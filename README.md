# Showcase
A portfolio repo for code samples

## [image_etl](image_etl/)

### Description
A repo containing code for a small, hypothetical ETL environment. The ETL goes out to imgur, pulls links to images with a certain, user defined tag, saves them locally, then transforms them to create two files in a subdirectory: a `raw` file which is the source image downloaded and a `thumbnail` image, which is a resized smaller version of the source image. Basic information is gathered at the time of ingestion and transformation and is then written to a Postgres database.

For the purposes of this example, the ETL pulls the most 'viral' images with the tag "cats" and saves them.

### Files

*[ImageClient.py](image_etl/ImageClient.py):*

A wrapper for the imgur python client. Allows a user to access the imgur API (using pre-established service account credentials). It allows the ETL to perform the following tasks:

* Connect to imgur API

* Pull a set of links to images with a certain tag

* Execute a multi-threaded download of all images in the generated list of links

* Save downloaded file to a local storage path


*[SQLEngine.py](image_etl/SQLEngine.py):*

Essentially a context handler for SQLAlchemy connections. This script is not new to this showcase repo, as I use it to communicate with my home Postgres server. It has been altered for this repo by adding docstrings and allowing the user to specify which database to connect to for each instance. 

This class/script exists because it allows the use of `with ...` with the db connection. Otherwise, I had to keep remembering to 'manually' close each connection. This handles that for me by generating the needed connection string, making the connection only during execution, and ensuring that the connection is closed. It exists because SQLAlchemy does not appear to have any sort of context management, so I made this to do that.


*[SQLUtils.py](image_etl/SQLUtils.py):*

A class that operates on top of the `SQLEngine` class and allows a user to more easily pass in parameters from Python to generate `SELECT`, `UPDATE`, and `DELETE` statements, which it then executes. This class is also not entirely new to this project, as I like to use it to update tables in Postgres. What is new is the [write_dataframe_safe](image_etl/SQLUtils.py#write_dataframe_safe) method which takes in a pandas dataframe and tries to write it to a specified database table. If it cannot, it will print out an error. This isn't totally necessary, but does lay the groundwork for better error handling when interacting with SQL.

*[Transformations.py](image_etl/Transformations.py):*

A collection of image transformation related methods to be called directly in the runner script. These include:

* Calculating image resize dimensions

* Creating a resized thumbnail copy of the source image

* Creating a new subdirectory for the transformed images to be saved in

* Generating new file names for transformed images

* Renaming the files and writing them to new destinations

* Removing the original image (assuming the transformations were successful

* Generating data from each transformation in the form of a pandas dataframe (which can/is then  written out to Postgres)

More transformation methods could be added as needed, or these could even be split up into sub scripts if the scope of transformations increases.

*[image_etl_runner.py](image_etl/image_etl_runner.py):*

The runner that executes the actual ETL. It pulls all the "cats" tagged images from Imgur and generates a dictionary of `{image_link: destination_path}`. It then iterates through the dictionary, using each destination path to generate the transformed images (renaming the source image by appending `_raw` to the file name and generating the thumbnail version) and pulls data from the transformation (image size on disk, image dimensions, storage location, image id, etc.) which is then appended to an output dataframe. Said dataframe is then written to a Postgres table.

*[sample_output.csv](image_etl/sample_output.csv):*

A sample of the data generated by this ETL for reference.


### Limitations

This etl only saves files locally. It could be modified to write out to a cloud storage location (S3) or a network storage drive. As it happens, the destination folder `/home/pi/c_drive` is actually a network hosted samba share.

Though the downloading task is multi-threaded, the rest of the ETL, namely the transformations, are not. A possible solution would be to alter the logic to execute the transformations as sub-processes. I did not implement this because the machine I used to create this repo has limited hardware and I didn't want to test its limits.


### Next Steps

Ideally I'd like to convert this using spark at least for the transformations. An obstacle with that appraoch could be that there aren't any spark libraries that handle processing images the way `PIL` does. One could be able to distribute a lot of the file handling, but the ETL would be bottle necked at the transformation steps, rendering spark's advantages relatively useless.

This leaves the multi-processing approach as the probable next best solution, but would have to be executed in an environment with more resources than my Raspberry Pi 4 (ARM CPU and 4 GB RAM total).

## [image_qc](image_qc/)

### Description
A local web app that reads in the data from the imgur [image ETL](image_etl/) and displays a preview image. The app also allows a user to select and zoom in on each image.

Can be deployed locally using the [runner file](image_qc/image_qc_runner.R) after sourcing required crednetials.

### Files

*[image_qc_runner.R](image_qc/image_qc_runner.R):*

A runner file that sets the host ip and port and then executes an instance of the app. Logging output is printed to terminal.

Usage:

`$ Rscript image_qc_runner.R`


*[image_qc_viewer/server.R](image_qc/image_qc_viewer/server.R):*

The server file for the shiny app. This contols the backend functions and interactions with the Postgres server. The workflow of the app is as follows:

* Pull in credentials from environment variable loaded prior to execution

* Pull the imgur ETL data from postgres

* Generate a global id variable `image_id` using the filename (sans file extension) of the first image returned from the db

* Cache the raw photo for better zoom creation later on

* Get file path to `_preview` image based on `image_id`

* Load the preview image and display it on the main page section

* Generate and display image statistics for the preview image and the underlying raw image

A user can select a section of the preview image which generates a "zoomed in" image below. The zoomed in version is generted as follows:

* Record pixel locations of the selected box

* Convert those values using the conversion factor from the db between the preview image's dimensions and the underlying raw image's dimensions

* Generate a `crop_string` based on the converted dimensions that can be used with `magick`'s `image_crop()` function

* Using the cached raw image, generate a cropped version using the converted dimensions of the slected box and save it locally as `tmp.jpg`

* Change the output of the main page of the app to include a second image viewing object under the preview image

When a user disconnects from the app, it ensures that any sql connection that might be active is disconnected and deletes `tmp.jpg`

*[image_qc_viewer/ui.R](image_qc/image_qc_viewer/ui.R):*

The ui file for the shiny app. This sets the front end aspects of the app, providing the placeholders for the dynamic image displays and file selection dropdown with `uiOutput()` objects. This app uses the `shinydashboard` package to wrap the UI and make it look a little bit more presentable.

*[image_qc_viewer/www/custom.css](image_qc/image_qc_viewer/www/custom.css):*

A CSS file to handle the look and aesthetic actions of the app. At this stage, it just ensures that the main page will allow a user to scroll to the left or right if a zoomed image is larger than the size of the screen. Other aesthetics can be changed here.


### Limitations

This app is incredibly limited and really only meant as a way to enhance the example of the imgur ETL and any other image related work. 

Shiny apps, and R in general, can only operate on a single threaded basis. Furthermore, this app is highly reliant on RAM which it has to share with the data it works with and especially with the images it processes. A better solution would need to be written in a more robust language and connected to a real webserver as opposed to just starting up the instance and shoving it into a specified port. 

The advantage of this app is that it is easy to code and able to get up and running, processing images and managing data connection, within a number of hours. It can also easily be deployed locally using simple bash commands or via docker.

This is not a wonderful production solution, but it is an incredible proof of concept tool.

