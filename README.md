# Showcase
A portfolio repo for code samples

## [image_etl](image_etl/)

### Description
A repo containing code for a small, hypothetical ETL environment. The ETL goes out to imgur, pulls links to images with a certain, user defined tag, saves them locally, then transforms them to create two files in a subdirectory: a `raw` file which is the source image downloaded and a `thumbnail` image, which is a resized smaller version of the source image. Basic information is gathered at the time of ingestion and transformation and is then written to a Postgres database.

For the purposes of this example, the ETL pulls the most 'viral' images with the tag "cats" and saves them.

### Files

*[ImageClient.py](image_etl/ImageClient.py):*

A wrapper for the imgur python client. Allows a user to access the imgur API (using pre-established service account credentials). It allows the ETL to perform the following tasks:

* Connect to imgur API

* Pull a set of links to images with a certain tag

* Execute a multi-threaded download of all images in the generated list of links

* Save downloaded file to a local storage path


*[SQLEngine.py](image_etl/SQLEngine.py):*

Essentially a context handler for SQLAlchemy connections. This script is not new to this showcase repo, as I use it to communicate with my home Postgres server. It has been altered for this repo by adding docstrings and allowing the user to specify which database to connect to for each instance. 

This class/script exists because it allows the use of `with ...` with the db connection. Otherwise, I had to keep remembering to 'manually' close each connection. This handles that for me by generating the needed connection string, making the connection only during execution, and ensuring that the connection is closed. It exists because SQLAlchemy does not appear to have any sort of context management, so I made this to do that.


*[SQLUtils.py](image_etl/SQLUtils.py):*

A class that operates on top of the `SQLEngine` class and allows a user to more easily pass in parameters from Python to generate `SELECT`, `UPDATE`, and `DELETE` statements, which it then executes. This class is also not entirely new to this project, as I like to use it to update tables in Postgres. What is new is the [write_dataframe_safe](image_etl/SQLUtils.py#write_dataframe_safe) method which takes in a pandas dataframe and tries to write it to a specified database table. If it cannot, it will print out an error. This isn't totally necessary, but does lay the groundwork for better error handling when interacting with SQL.

*[Transformations.py](image_etl/Transformations.py):*

A collection of image transformation related methods to be called directly in the runner script. These include:

* Calculating image resize dimensions

* Creating a resized thumbnail copy of the source image

* Creating a new subdirectory for the transformed images to be saved in

* Generating new file names for transformed images

* Renaming the files and writing them to new destinations

* Removing the original image (assuming the transformations were successful

* Generating data from each transformation in the form of a pandas dataframe (which can/is then  written out to Postgres)

More transformation methods could be added as needed, or these could even be split up into sub scripts if the scope of transformations increases.

*[image_etl_runner.py](image_etl/image_etl_runner.py):*

The runner that executes the actual ETL. It pulls all the "cats" tagged images from Imgur and generates a dictionary of `{image_link: destination_path}`. It then iterates through the dictionary, using each destination path to generate the transformed images (renaming the source image by appending `_raw` to the file name and generating the thumbnail version) and pulls data from the transformation (image size on disk, image dimensions, storage location, image id, etc.) which is then appended to an output dataframe. Said dataframe is then written to a Postgres table.

*[sample_output.csv](image_etl/sample_output.csv):*

A sample of the data generated by this ETL for reference.


### Limitations

This etl only saves files locally. It could be modified to write out to a cloud storage location (S3) or a network storage drive. As it happens, the destination folder `/home/pi/c_drive` is actually a network hosted samba share.

Though the downloading task is multi-threaded, the rest of the ETL, namely the transformations, are not. A possible solution would be to alter the logic to execute the transformations as sub-processes. I did not implement this because the machine I used to create this repo has limited hardware and I didn't want to test its limits.


### Next Steps

Ideally I'd like to convert this using spark at least for the transformations. An obstacle with that appraoch could be that there aren't any spark libraries that handle processing images the way `PIL` does. One could be able to distribute a lot of the file handling, but the ETL would be bottle necked at the transformation steps, rendering spark's advantages relatively useless.

This leaves the multi-processing approach as the probable next best solution, but would have to be executed in an environment with more resources than my Raspberry Pi 4 (ARM CPU and 4 GB RAM total).
